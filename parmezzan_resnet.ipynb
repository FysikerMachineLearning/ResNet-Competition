{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet-Competition\n",
    "\n",
    "__Author:Marcus P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: MLP Conf: [784, 13, 10, 10, 10] test acc 0.9726 Stopped at: 11\n",
      "Name: RES_NET Conf: [784, 12, 12, 12, 10] test acc 0.9483 Stopped at: 21\n",
      "Name: MLP Conf: [784, 12, 10, 10, 10, 10, 10, 10] test acc 0.9752 Stopped at: 16\n",
      "Name: RES_NET Conf: [784, 11, 11, 11, 11, 11, 11, 10] test acc 0.9436 Stopped at: 21\n",
      "Name: MLP Conf: [784, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10] test acc 0.9725 Stopped at: 22\n",
      "Name: RES_NET Conf: [784, 11, 11, 11, 11, 11, 11, 11, 11, 11, 10] test acc 0.9477 Stopped at: 20\n",
      "Name: MLP Conf: [784, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10] test acc 0.9712 Stopped at: 26\n",
      "Name: RES_NET Conf: [784, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10] test acc 0.9364 Stopped at: 23\n"
     ]
    }
   ],
   "source": [
    "import numpy as np;\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense, Flatten,Input\n",
    "from keras.models import Model\n",
    "from keras.layers.merge import add\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf;\n",
    "\n",
    "\n",
    "def build_res_net(n_layers=3,activation_fun=tf.nn.swish,max_number_of_weights=10**4, w=28,h=28, n_out=10):\n",
    "    #(w*h * size +  n_layers *size^2 + size*n_out) < max_number_of_weights\n",
    "    #((w*h + n_out) * size/n_layers +  size^2 -max_number_of_weights/n_layers  = 0;\n",
    "    pdiv2 =  (w*h + n_out)/(2*n_layers);\n",
    "    res_net_size = int(np.floor( -pdiv2 + np.sqrt(pdiv2**2 + max_number_of_weights/n_layers)));\n",
    "    \n",
    "    # The model\n",
    "    input_tensor = Input(shape=(1,w,h))\n",
    "    x = Flatten()(input_tensor)\n",
    "    x = Dense(res_net_size, activation=activation_fun)(x)\n",
    "    layer_sizes = []; \n",
    "    layer_sizes.append(w*h);\n",
    "    for i in range(n_layers):\n",
    "        layer_sizes.append(res_net_size);\n",
    "\n",
    "        x = res_block(res_net_size,activation_fun)(x)\n",
    "    x = Dense(units=n_out,activation='softmax')(x)\n",
    "    layer_sizes.append(n_out);\n",
    "\n",
    "    model = Model(inputs=input_tensor, outputs=x)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model,layer_sizes,'RES_NET';\n",
    "def build_mlp(n_layers=3,activation_fun=tf.nn.swish,max_number_of_weights=10**4, w=28,h=28, n_out=10):\n",
    "    # exp choose sizes\n",
    "    # choose appropiate  slope\n",
    "    # e^(-(k*x-s)) + m = input_size, if x = 0\n",
    "    # e^(-(k*x-s)) + m = output_size, if x = n_layers +1;\n",
    "    # ---- > input_size - e^(s) = m --> e^(-(k*(n_layers +1)-s)) + input_size - e^(s) = output_size\n",
    "    # ----> e^(s)(1/e^(k*(n_layers +1))  - 1) = output_size - input_size\n",
    "    # ---> s = log((output_size - input_size)/(1/e^(k*(n_layers +1))  - 1))  \n",
    "    num_weights = 100000000;\n",
    "    # a stupid way to get the number of weights for each layer\n",
    "    k = 0.5; #could be a bad initial guess\n",
    "    k_step = 0.01;\n",
    "    n_in = w*h;\n",
    "    visited_k  = set();\n",
    "    layer_sizes = [];\n",
    "    while True:\n",
    "        s = np.log((n_out - n_in)/(1/np.exp(k*(n_layers +1))  - 1));\n",
    "        m = n_in - np.exp(s);\n",
    "    \n",
    "        layer_sizes = [];\n",
    "        for i in range(n_layers+2):\n",
    "            layer_sizes.append(int(round(np.exp(-(k*i -s))+ m)));\n",
    "        num_weights = 0;\n",
    "        for i in range(len(layer_sizes) -1):\n",
    "            num_weights += layer_sizes[i+1]*layer_sizes[i];\n",
    "   \n",
    "        if(num_weights > max_number_of_weights):\n",
    "            k += k_step;\n",
    "        else:\n",
    "            visited_k.add(k);\n",
    "            k -= k_step;\n",
    "        if k in visited_k:\n",
    "            break;\n",
    "    \n",
    "    # The model\n",
    "    input_tensor = Input(shape=(1,w,h))\n",
    "    x = Flatten()(input_tensor)\n",
    "    for i in range(len(layer_sizes)-1):\n",
    "        x = Dense(layer_sizes[i], activation=activation_fun)(x)\n",
    "    x = Dense(units=n_out,activation='softmax')(x)\n",
    "    model = Model(inputs=input_tensor, outputs=x)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model,layer_sizes,'MLP';\n",
    "def res_block(n_output,activation_fun):\n",
    "\n",
    "    def f(x):\n",
    "        \n",
    "        # H_l(x):\n",
    "        # first pre-activation\n",
    "        h =Dense(n_output, activation=activation_fun)(x)\n",
    "        return add([x, h])\n",
    "    return f\n",
    "\n",
    "\n",
    "\n",
    "# Game rules\n",
    "max_number_of_weights=10**4\n",
    "\n",
    "#Some conf.\n",
    "n_layers_interv = [3,6,9,12];\n",
    "\n",
    "\n",
    "#load data \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28).astype('float32')\n",
    "\n",
    "train_x =X_train / 255\n",
    "test_x =X_test / 255\n",
    "train_y = np_utils.to_categorical(y_train)\n",
    "test_y = np_utils.to_categorical(y_test)\n",
    "\n",
    "\n",
    "# training\n",
    "for n_layers in n_layers_interv:\n",
    "    models = [];\n",
    "    models.append(build_mlp(n_layers=n_layers,max_number_of_weights=max_number_of_weights));\n",
    "    models.append(build_res_net(n_layers=n_layers));\n",
    "    \n",
    "    for i in range(len(models)):\n",
    "        model = models[i][0];\n",
    "        max_epochs = 300\n",
    "        \n",
    "        #Store training stats\n",
    "        early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n",
    "        \n",
    "        \n",
    "        history = model.fit(train_x,train_y, epochs=max_epochs,\n",
    "                           validation_split=0.2, verbose=0,shuffle=False,\n",
    "                           callbacks=[early_stop],batch_size=100) \n",
    "        \n",
    "        \n",
    "        [loss, acc] = model.evaluate(test_x, test_y, verbose=0)\n",
    "        \n",
    "        print(\"Name: \"+ models[i][2] + \" Conf: \" +  str(models[i][1]) + \" test acc \"  + str(acc) +\" Stopped at: \" + str(len(history.history['val_loss'])) );\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
